# ðŸš€ Day 8 â€“ Embeddings Deep Dive (GenAI Roadmap)

## ðŸ“Œ Overview
Day 8 focuses on understanding **text embeddings**, semantic similarity, and how vectors capture meaning in natural language.

Embeddings are the foundation of:
- Semantic Search
- RAG (Retrieval-Augmented Generation)
- Chatbot Memory
- Recommendation Systems
- Clustering & Similarity Matching

---

## ðŸ§  What I Learned

### ðŸ”¹ What Are Embeddings?
Embeddings are dense numerical vector representations of text that capture semantic meaning.

Example:
"I love AI" â†’ `[0.12, -0.98, 0.44, ...]`

Similar meaning â†’ vectors close in space  
Different meaning â†’ vectors far apart  

---

### ðŸ”¹ Semantic Similarity
Sentences with similar meaning have high cosine similarity.

Example:
- "I love machine learning"
- "I enjoy studying AI"

These produce high similarity scores (~0.8+).

---

### ðŸ”¹ Why Keywords Fail
Traditional keyword search fails because:
- No synonym understanding
- No context awareness
- No paraphrase detection

Example:
Query: "cheap smartphone"
Document: "Affordable mobile phone under budget"

Keyword search â†’ fails  
Embeddings â†’ succeeds  

---

## ðŸ›  Implementation

### Model Used
`all-MiniLM-L6-v2` from Sentence Transformers

### Installation

```bash
pip install sentence-transformers